{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/pgm')\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.transforms import Transform\n",
    "from torch.distributions import constraints\n",
    "from torch import Tensor\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions.conditional import (\n",
    "    ConditionalTransformModule,\n",
    "    ConditionalTransformedDistribution\n",
    ")\n",
    "\n",
    "from flow_pgm import BasePGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Causal Mechanisms\n",
    "\n",
    "This notebook presents a discrete causal mechanism (i.e. continuous cause(s) and discrete effect) using a Gumbel-Softmax/Concrete TransformedDistribution. It works by applying a special Softmax bijector (i.e. SoftmaxCentered) to a Gumbel source distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = torch.tensor(data[\"data\"]).float()\n",
    "Y = torch.tensor(data[\"target\"])\n",
    "\n",
    "def min_max_scale(X: Tensor):\n",
    "    X_min = X.min(dim=0).values\n",
    "    return 2 * (X - X_min) / (X.max(dim=0).values - X_min) - 1  # [-1, 1]\n",
    "X = min_max_scale(X)\n",
    "\n",
    "N, M = X.shape\n",
    "K = 2  # simple binary classification task\n",
    "split = int(0.8 * N)\n",
    "# NOTE: one-hot encoding to K+1 for training under a softmax bijection (see SoftmaxCentered)\n",
    "x_train, y_train = X[:split], F.one_hot(Y[:split], num_classes=K + 1).float()\n",
    "x_valid, y_valid = X[split:], F.one_hot(Y[split:], num_classes=K + 1).float()\n",
    "\n",
    "eps = 1e-3  # option to relax discrete labels\n",
    "y_train = y_train * (1 - eps) + eps / (K + 1)\n",
    "y_valid = y_valid * (1 - eps) + eps / (K + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 17.3793\n",
      "train acc: 0.4615, valid acc: 0.4386\n",
      "epoch 50: loss = -14.4376\n",
      "train acc: 0.5934, valid acc: 0.4649\n",
      "epoch 100: loss = -16.4228\n",
      "train acc: 0.7055, valid acc: 0.6842\n",
      "epoch 150: loss = -19.1945\n",
      "train acc: 0.9231, valid acc: 0.8333\n",
      "epoch 200: loss = -21.5080\n",
      "train acc: 0.9297, valid acc: 0.9561\n",
      "epoch 250: loss = -22.3139\n",
      "train acc: 0.9560, valid acc: 0.9386\n",
      "epoch 300: loss = -22.2545\n",
      "train acc: 0.9451, valid acc: 0.9211\n",
      "epoch 350: loss = -24.6088\n",
      "train acc: 0.9802, valid acc: 0.9649\n",
      "epoch 400: loss = -26.7565\n",
      "train acc: 0.9736, valid acc: 0.9561\n",
      "epoch 450: loss = -27.4272\n",
      "train acc: 0.9692, valid acc: 0.9737\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxCentered(Transform):\n",
    "    \"\"\"\n",
    "    Implements softmax as a bijection, the forward transformation appends a value to the\n",
    "    input and the inverse removes it. The appended coordinate represents a pivot, e.g., \n",
    "    softmax(x) = exp(x-c) / sum(exp(x-c)) where c is the implicit last coordinate.\n",
    "\n",
    "    Adapted from a Tensorflow implementation: https://tinyurl.com/48vuh7yw \n",
    "    \"\"\"\n",
    "    domain = constraints.real_vector\n",
    "    codomain = constraints.simplex\n",
    "\n",
    "    def __init__(self, temperature: float = 1.):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        zero_pad = torch.zeros(*x.shape[:-1], 1, device=x.device)\n",
    "        x_padded = torch.cat([x, zero_pad], dim=-1)\n",
    "        return (x_padded / self.temperature).softmax(dim=-1)\n",
    "\n",
    "    def _inverse(self, y: Tensor):\n",
    "        log_y = torch.log(y.clamp(min=1e-12))\n",
    "        unorm_log_probs = log_y[..., :-1] - log_y[..., -1:]\n",
    "        return unorm_log_probs * self.temperature\n",
    "\n",
    "    # def log_abs_det_jacobian(self, x: Tensor, y: Tensor): \n",
    "    #     \"\"\" -log|det(dx/dy)| \"\"\"\n",
    "    #     Kplus1 = torch.tensor(x.size(-1) + 1, dtype=x.dtype, device=x.device)\n",
    "    #     return 0.5 * kp1.log() + torch.sum(x, dim=-1) - \\\n",
    "    #         Kplus1 * F.softplus(torch.logsumexp(x, dim=-1))\n",
    "\n",
    "    def log_abs_det_jacobian(self, x: Tensor, y: Tensor):\n",
    "        \"\"\" log|det(dy/dx)| \"\"\"\n",
    "        Kplus1 = torch.tensor(y.size(-1), dtype=y.dtype, device=y.device)\n",
    "        return 0.5 * Kplus1.log() + torch.sum(torch.log(y.clamp(min=1e-12)), dim=-1)\n",
    "\n",
    "    def forward_shape(self, shape: torch.Size):\n",
    "        return shape[:-1] + (shape[-1] + 1,)  # forward appends one dim\n",
    "\n",
    "    def inverse_shape(self, shape: torch.Size):\n",
    "        if shape[-1] <= 1:\n",
    "            raise ValueError\n",
    "        return shape[:-1] + (shape[-1] - 1,)  # inverse removes last dim\n",
    "\n",
    "\n",
    "class ConditionalAffineTransform(ConditionalTransformModule):\n",
    "    def __init__(self, context_nn: nn.Module, event_dim: int = 0, **kwargs: Any):\n",
    "        super().__init__(**kwargs)\n",
    "        self.event_dim = event_dim\n",
    "        self.context_nn = context_nn\n",
    "\n",
    "    def condition(self, context: Tensor):\n",
    "        loc, log_scale = self.context_nn(context)\n",
    "        return torch.distributions.transforms.AffineTransform(\n",
    "            loc, F.softplus(log_scale), event_dim=self.event_dim\n",
    "        )\n",
    "\n",
    "class BreastPGM(BasePGM):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_dims: List[int] = [128, 128], \n",
    "        num_inputs: int = 30, \n",
    "        num_classes: int = 2,\n",
    "        temperature: float = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.variables = {\"x\": \"continuous\", \"y\": \"binary\"}\n",
    "        self.affine_transform = ConditionalAffineTransform(\n",
    "            pyro.nn.DenseNN(\n",
    "                num_inputs, \n",
    "                hidden_dims, \n",
    "                param_dims=[num_classes, num_classes], \n",
    "                nonlinearity=nn.ReLU()),\n",
    "            event_dim=1\n",
    "        )\n",
    "        self.softmax_transform = SoftmaxCentered(temperature)\n",
    "        self.x_loc = nn.Parameter(torch.zeros(num_inputs))\n",
    "        self.x_logscale = nn.Parameter(torch.zeros(num_inputs))\n",
    "        self.register_buffer(\"y_base_loc\", torch.zeros(num_classes))\n",
    "        self.register_buffer(\"y_base_scale\", torch.ones(num_classes))\n",
    "\n",
    "    def model(self):\n",
    "        pyro.module(\"PGM\", self)\n",
    "        cause = pyro.sample(\"x\", dist.Normal(\n",
    "                self.x_loc, F.softplus(self.x_logscale, beta=np.log(2))\n",
    "            ).to_event(1)\n",
    "        )\n",
    "        base_dist = dist.Gumbel(self.y_base_loc, self.y_base_scale).to_event(1)\n",
    "        flow_dist = ConditionalTransformedDistribution(\n",
    "            base_dist, [self.affine_transform, self.softmax_transform],\n",
    "        ).condition(cause)\n",
    "        effect = pyro.sample(\"y\", flow_dist)\n",
    "        return {\"x\": cause, \"y\": effect}\n",
    "    \n",
    "    def cond_model(self, obs: Dict[str, Tensor]):\n",
    "        with pyro.plate(\"obs\", obs[\"x\"].shape[0]):\n",
    "            pyro.condition(self.model, data=obs)()\n",
    "    \n",
    "    def guide_pass(self, obs: Any):\n",
    "        pass\n",
    "\n",
    "    def predict_y(self, obs: Dict[str, Tensor]):\n",
    "        cond_model = pyro.condition(self.model, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_model).get_trace()\n",
    "        return cond_trace.nodes[\"y\"][\"value\"].argmax(-1)\n",
    "\n",
    "model = BreastPGM(hidden_dims=[128, 128], num_inputs=M, num_classes=K, temperature=1)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "elbo_fn = pyro.infer.Trace_ELBO()\n",
    "pyro.clear_param_store()\n",
    "x_train, y_train = x_train.cuda(), y_train.cuda() \n",
    "x_valid, y_valid = x_valid.cuda(), y_valid.cuda()\n",
    "\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    loss = elbo_fn.differentiable_loss(\n",
    "        model.cond_model,\n",
    "        model.guide_pass, \n",
    "        {\"x\": x_train, \"y\": y_train}\n",
    "    ) / x_train.shape[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch {epoch}: loss = {loss.item():.4f}')\n",
    "        with torch.no_grad():\n",
    "            train_acc = (model.predict_y({\"x\": x_train}) == y_train.argmax(-1)).float().mean()       \n",
    "            valid_acc = (model.predict_y({\"x\": x_valid}) == y_valid.argmax(-1)).float().mean()\n",
    "            print(f'train acc: {train_acc.item():.4f}, valid acc: {valid_acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_valid = x_valid.shape[0]\n",
    "counterfactuals = model.counterfactual(\n",
    "    obs={\"x\": x_valid, \"y\": y_valid},\n",
    "    intervention={\"x\": 2 * torch.rand_like(x_valid) - 1}, # randomly intervene on cause\n",
    ")\n",
    "assert N_valid != (counterfactuals[\"y\"].argmax(-1) == y_valid.argmax(-1)).sum().item()\n",
    "\n",
    "counterfactuals = model.counterfactual(\n",
    "    obs={\"x\": x_valid, \"y\": y_valid},\n",
    "    intervention={\"y\": y_valid}, # \"do nothing\"\n",
    ")\n",
    "assert N_valid == (counterfactuals[\"y\"].argmax(-1) == y_valid.argmax(-1)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[1.3999e-02, 9.8567e-01, 3.2696e-04]], device='cuda:0')\n",
      "f_inv: _InverseTransform(SoftmaxCentered()), x: tensor([[3.7569, 8.0112]], device='cuda:0')\n",
      "f_inv: _InverseTransform(AffineTransform()), x: tensor([[-0.2828,  0.6130]], device='cuda:0')\n",
      "x: tensor([[-0.2828,  0.6130]], device='cuda:0')\n",
      "f: AffineTransform(), y: tensor([[3.7569, 8.0112]], device='cuda:0')\n",
      "f: SoftmaxCentered(), y: tensor([[1.3999e-02, 9.8567e-01, 3.2696e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "self = model\n",
    "base_dist = dist.Gumbel(self.y_base_loc, self.y_base_scale).to_event(1)\n",
    "flow_dist = ConditionalTransformedDistribution(\n",
    "    base_dist, [self.affine_transform, self.softmax_transform],\n",
    ").condition(x_valid[:N])\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = flow_dist.sample()\n",
    "    init_y = y.clone()\n",
    "    print(f'y: {init_y}')\n",
    "    for fn in reversed(flow_dist.transforms):\n",
    "        x = fn.inv(y)\n",
    "        print(f'f_inv: {fn.inv}, x: {x}')\n",
    "        y = x\n",
    "\n",
    "    print(f'x: {x}')\n",
    "    for fn in flow_dist.transforms:\n",
    "        y = fn(x)\n",
    "        print(f'f: {fn}, y: {y}')\n",
    "        x = y\n",
    "    assert torch.allclose(init_y, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
